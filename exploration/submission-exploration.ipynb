{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most reccomeneded books on Reddit\n",
    "\n",
    "People often use Amazon links (to books) in Reddit comments to act as a proxy for what books are being mentioned the most on Reddit. This is due to the fact that Amazon links are easy to parse (see regex) and look up (see Amazon PA-API). However, this isn't necessarily an accurate proxy, as there are countless mentions to books by just using the title and author (e.g. *The Intelligent Investor by Benjamin Graham*).\n",
    "\n",
    "Many of these book mentions come from submissions asking questionss such as these:\n",
    "* [Reddit, what are some \"MUST read\" books?](https://www.reddit.com/r/AskReddit/comments/34m5n6/reddit_what_are_some_must_read_books/)\n",
    "* [What are /r/investing's favorite books? - Future side bar link.](https://www.reddit.com/r/investing/comments/166ha8/what_are_rinvestings_favorite_books_future_side/)\n",
    "* [What is a good cook book for a beginner?](https://www.reddit.com/r/Cooking/comments/6m5enh/what_is_a_good_cook_book_for_a_beginner/)\n",
    "\n",
    "Taking a brief look at these posts, there are almost no Amazon links, and consequently modern scrapers will not pick up these book reccomendations. Even more, these posts are highly targeted, and garner attention from the entire community--often providing hundreds of book reccomendations with in-depth discussions for each one. To miss out on these would be very detrimental to a reccomendation service that strives to be accurate.\n",
    "\n",
    "**Our goal in this notebook is to find a reliable method capable of finding which books were mentioned in a comment.** Here are some observations that may lead to such an algortihm:\n",
    "- Books are almost always mentioned in the top-level comments (in the kind of submissions mentioned above)\n",
    "- Most people capitalize the book title\n",
    "- Most people mention the author\n",
    "    - e.g. The Intelligent Investor **by Benjamin Graham**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base imports\n",
    "import praw\n",
    "import requests\n",
    "import time\n",
    "from urllib.error import HTTPError, URLError\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some sample comments to work with\n",
    "In order to first build and test our parser, we will gather sample comments from two subreddits that are dedicated soley to suggseting books:\n",
    "* [r/SuggestMeABook](https://reddit.com/r/suggestmeabook)\n",
    "* [r/booksuggestions](https://reddit.com/r/booksuggestions)\n",
    "\n",
    "We will futher refine our sample by only gathering comments from submissions that are asking a question (i.e. the title ends with a '?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_client():\n",
    "    api_creds = {}\n",
    "    with open('../puller-api-creds.env') as f:\n",
    "        for line in f:\n",
    "            k, v = line.rstrip().split('=')\n",
    "            api_creds[k] = v\n",
    "    return praw.Reddit( user_agent='book submission parser',\n",
    "                        client_id=api_creds['CLIENT_ID'],\n",
    "                        client_secret=api_creds['CLIENT_SECRET'],\n",
    "                        username=api_creds['USERNAME'],\n",
    "                        password=api_creds['PASSWORD'] )\n",
    "\n",
    "\n",
    "def sub_exists(reddit, subreddit):\n",
    "    from prawcore import NotFound\n",
    "    exists = True\n",
    "    try:\n",
    "        reddit.subreddits.search_by_name(subreddit, exact=True)\n",
    "    except NotFound:\n",
    "        exists = False\n",
    "    return exists\n",
    "\n",
    "\n",
    "def get_subreddit_sample_comments(reddit, subreddit_name):\n",
    "    if not sub_exists(reddit, subreddit_name):\n",
    "        raise ValueError(\"please enter a valid subreddit name\")\n",
    "    comments = []\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for submission in subreddit.top(time_filter='week'):\n",
    "        if submission.title[-1] != '?':\n",
    "            continue\n",
    "        comments.extend(submission.comments)\n",
    "        break\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the client\n",
    "reddit = get_reddit_client()\n",
    "# gather some arbritrary sample comments\n",
    "comments = get_subreddit_sample_comments(reddit, 'suggestmeabook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered 68 sample comments\n",
      "--------------------------------------------------------------------------------\n",
      "*The Painted Bird* by Kozinski should make you lose faith in humanity.\n",
      "--------------------------------------------------------------------------------\n",
      "The Collector by John Fowles\n",
      "\n",
      "It was unsettling how 'normal' the characters thoughts and actions became as you kept reading \n",
      "--------------------------------------------------------------------------------\n",
      "The Conspiracy Against The Human Race by Thomas Ligotti. I swear to god this book is dangerous, I read it for the first time when I was already in a really nihilistic, existentially distraught place and it pretty much confirmed and enforced the way I was feeling. I am still and always will be a big Ligotti fan but I do wonder if the last 5 years would have been different if I didnâ€™t go into such a downward spiral at such a crucial time in my life.\n"
     ]
    }
   ],
   "source": [
    "print(f'Gathered {len(comments)} sample comments')\n",
    "for comment in comments[:3]:\n",
    "    print('-' * 80)\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a preliminary parsing pipeline\n",
    "The stages of parsing book titles out of a comment will be broken into the following steps:\n",
    "\n",
    "**Important Note**: Comments on Reddit are represented by markdown\n",
    "\n",
    "1. Get the text-representation of the rendered markdown \n",
    "2. Tokenize the comment text into sentences\n",
    "3. Tokenize each sentence into words\n",
    "4. Find consecutive sequences of capitalized non-stopword words\n",
    "    * e.g. I thought that *The Intelligent Investor: The Definitive Book on Value Investing* was a great book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "\n",
    "DEBUG = False\n",
    "# Really abritrary title length requirement\n",
    "MIN_TITLE_LEN = 4\n",
    "# Global stopwords set (for quick lookup)\n",
    "STOPWORDS = set(stopwords.words())\n",
    "\n",
    "def markdown_to_text(md):\n",
    "    html = markdown(md)\n",
    "    return ''.join(BeautifulSoup(html, 'lxml').findAll(text=True))\n",
    "\n",
    "\n",
    "def _trim_trailing_stopwords(words):\n",
    "    while words and words[-1] in STOPWORDS:\n",
    "        words.pop()\n",
    "    return words\n",
    "\n",
    "\n",
    "def _match_titles(sentence):\n",
    "    \"\"\"\n",
    "    Return title(s) found in a sentence, where a title is defined as:\n",
    "        a consecutive sequence of capitalized non-stopword words\n",
    "    \"\"\"\n",
    "    titles, seq = [], []\n",
    "    # filter out the special chars, e.g. ''', '\"', ',', etc.\n",
    "    words = filter(lambda w: w.isalnum(), word_tokenize(sentence))\n",
    "    for word in words:\n",
    "        # title 'ends' on a non-stopword non-capitalized word\n",
    "        if seq and word not in STOPWORDS and not word[0].isupper():\n",
    "            titles.append(seq[:])\n",
    "            seq = []\n",
    "        elif seq and word in STOPWORDS:\n",
    "            seq.append(word)\n",
    "        elif word[0].isupper():\n",
    "            seq.append(word)\n",
    "            \n",
    "    titles.append(seq)\n",
    "    trimmed = map(_trim_trailing_stopwords, titles)\n",
    "    filtered = filter(lambda l: len(l) >= MIN_TITLE_LEN, trimmed)\n",
    "    return [' '.join(title) for title in filtered if title]\n",
    "        \n",
    "\n",
    "def extract_titles_from_comment(comment):\n",
    "    \"\"\" \n",
    "    Extracts all book titles found in a comment body\n",
    "    See the pipeline steps mentioned in the cell above.\n",
    "    \"\"\"\n",
    "    titles = set()\n",
    "    # avoid dealing with all the special markup characters\n",
    "    text = markdown_to_text(comment.body)\n",
    "    # for each sentence, extract the title(s)\n",
    "    for sentence in sent_tokenize(text):\n",
    "        \n",
    "        titles_found = _match_titles(sentence)\n",
    "        if DEBUG and not titles_found:\n",
    "            print(f'sentence: {sentence}')\n",
    "            print('titles found:')\n",
    "            print(\"\\n\".join(titles_found))\n",
    "            print('-' * 80)\n",
    "        titles.update(titles_found)\n",
    "    return titles\n",
    "\n",
    "\n",
    "def bulk_extract(comments):\n",
    "    \"\"\" \"\"\"\n",
    "    all_titles = set()\n",
    "    for comment in comments:\n",
    "        titles = extract_titles_from_comment(comment)\n",
    "        all_titles.update(titles)\n",
    "    return list(all_titles)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Laws of Nature by Ashley Franz Holzmann',\n",
       " 'The Surgeon Tess Gerritsen',\n",
       " 'Blood Meridian by Cormac McCarthy',\n",
       " 'Flowers In The Attic VC Andrews',\n",
       " 'The Tsar of Love and Techno by Anthony Marra',\n",
       " 'Eleven Twenty Three by Jason Hornsby Preta Realm by J Thorn A',\n",
       " 'Also Lull by Kelly Link',\n",
       " 'Twenty Days of Turin The Water Knife',\n",
       " 'The Stuff of Nightmares by Malorie Blackman',\n",
       " 'Slade House by David Mitchell']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_extracted_titles = bulk_extract(comments)\n",
    "sample_extracted_titles[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging the Google books API to get book metadata from just the title\n",
    "Using Google's API provides the simplest method, however there is a cap at 1000 requests per day, so it must be used sparingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_BOOKS_API_URL = 'https://www.googleapis.com/books/v1/volumes'\n",
    "\n",
    "def google_book_search(title):\n",
    "    \"\"\" https://developers.google.com/books/docs/v1/using#PerformingSearch \"\"\"\n",
    "    params = {\n",
    "        'q': title,\n",
    "        'key': 'AIzaSyAgwbY2ojVCKMnnxoua7QJ0aYiYJxePmcQ',\n",
    "        'maxResults': 1,\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(GOOGLE_BOOKS_API_URL, params=params).json()\n",
    "        if 'items' not in resp:\n",
    "            print(f'no google search results found for title: {title}')\n",
    "            resp = None  # nothing really to work with when 0 items returend\n",
    "        return resp\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "def google_metadata_from_title(title):\n",
    "    \"\"\"\n",
    "    Get the most relevant search result from Google books API and returns a dict of metadata:\n",
    "    {'isbn': <isbn>, 'title': <title>}\n",
    "    \"\"\"\n",
    "    resp = google_book_search(title)\n",
    "    if not resp:\n",
    "        return\n",
    "    try:\n",
    "        metadata = resp['items'][0]['volumeInfo']\n",
    "        ids = metadata['industryIdentifiers']  # isbn10, isbn13, etc.\n",
    "        \n",
    "        return {\n",
    "            'isbn': next(d['identifier'] for d in ids if d['type'] == 'ISBN_10'), \n",
    "            'title': metadata['title'],\n",
    "            'authors': metadata['authors']\n",
    "        }\n",
    "    except StopIteration:\n",
    "        print(f'incompatible google search result format for title: {title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Google API results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted title: The Laws of Nature by Ashley Franz Holzmann\n",
      "Matched title: The Laws of Nature by Ashley Franz Holzmann\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: The Surgeon Tess Gerritsen\n",
      "Matched title: The Surgeon by Tess Gerritsen\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: Blood Meridian by Cormac McCarthy\n",
      "Matched title: Blood Meridian by Cormac McCarthy\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: Flowers In The Attic VC Andrews\n",
      "Matched title: Flowers In The Attic by V.C. Andrews\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: The Tsar of Love and Techno by Anthony Marra\n",
      "Matched title: The Tsar of Love and Techno by Anthony Marra\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: Eleven Twenty Three by Jason Hornsby Preta Realm by J Thorn A\n",
      "Matched title: The Cult of the Amateur by Andrew Keen\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: Also Lull by Kelly Link\n",
      "Matched title: Magic for Beginners by Kelly Link\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: Twenty Days of Turin The Water Knife\n",
      "Matched title: Hunters & Collectors by M. Suddain\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: The Stuff of Nightmares by Malorie Blackman\n",
      "Matched title: The Stuff of Nightmares by Malorie Blackman\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted title: Slade House by David Mitchell\n",
      "Matched title: Slade House by David Mitchell\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "google_results = []\n",
    "for title in sample_extracted_titles[:10]:\n",
    "    print(f'Extracted title: {title}')\n",
    "    result = google_metadata_from_title(title)\n",
    "    google_results.append(result if result else {})\n",
    "    if not result:\n",
    "        continue\n",
    "    print(f'Matched title: {result[\"title\"]} by {\", \".join(result[\"authors\"])}')\n",
    "    print('-' * 80)\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging the Goodread's API to get book metadata from just the title\n",
    "The goodreads API is slightly more involved, as title searches only returns Goodreads internal book id, which you must then translate to an ISBN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODREADS_SEARCH_API_URL = 'https://www.goodreads.com/search/index.xml'\n",
    "def goodreads_title_search(title):\n",
    "    \"\"\" https://www.goodreads.com/api/index#search.books \"\"\"\n",
    "    params = {\n",
    "        'q': title,\n",
    "        'key': 'CZ44l5tAA26Dp2hGQywKg',\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(GOODREADS_SEARCH_API_URL, params=params)\n",
    "        xml = BeautifulSoup(resp.text, 'xml')\n",
    "        if not xml.find('results') or not xml.find('results').find('work'):\n",
    "            xml = None  # no results is essnetially useless\n",
    "        return xml\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "GOODREADS_SHOW_API_URL = 'https://www.goodreads.com/book/show'\n",
    "def goodreads_show_by_id(book_id):\n",
    "    \"\"\" \n",
    "    https://www.goodreads.com/api/index#book.show \n",
    "    Lookup book reviews and metadata by goodread's interal book id\n",
    "    \"\"\"\n",
    "    endpoint = f'{book_id}.xml'  # they use a very weird endpoint format\n",
    "    params = {\n",
    "        'key': 'CZ44l5tAA26Dp2hGQywKg'\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(f'{GOODREADS_SHOW_API_URL}/{endpoint}', params=params)\n",
    "        xml = BeautifulSoup(resp.text, 'xml')\n",
    "        if not xml.find('book'):\n",
    "            xml = None\n",
    "        return xml\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    \n",
    "def goodreads_id_from_title(title):\n",
    "    \"\"\"  \"\"\"\n",
    "    resp = goodreads_title_search(title)\n",
    "    if not resp:\n",
    "        print(f'no goodreads book search results returned for title: {title}')\n",
    "        return\n",
    "    try:\n",
    "        work = resp.find('results').find('work')\n",
    "        if work.find('best_book_id'):\n",
    "            return work.find('best_book_id').text\n",
    "        else:\n",
    "            return work.find('best_book').find('id').text\n",
    "    except AttributeError:\n",
    "        print(f'incomplete goodreads book API response for title: {title}')\n",
    "        return\n",
    "    \n",
    "\n",
    "def goodreads_isbn_from_id(book_id):\n",
    "    resp = goodreads_show_by_id(book_id)\n",
    "    if not resp:\n",
    "        print(f'no goodreads books returned for goodreads book id: {book_id}')\n",
    "        return\n",
    "    try:\n",
    "        isbn = resp.find('isbn').text\n",
    "        if ' ' in isbn:\n",
    "            return None\n",
    "        return isbn\n",
    "    except AttributeError as e:\n",
    "        print(f'no isbn contained in response for goodreads book id: {book_id}')\n",
    "        \n",
    "\n",
    "def goodreads_isbn_from_title(title):\n",
    "    goodreads_id = goodreads_id_from_title(title)\n",
    "    if not goodreads_id:\n",
    "        return\n",
    "    isbn = goodreads_isbn_from_id(goodreads_id)\n",
    "    return isbn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing problems\n",
    "* Comments may mention many books, separated by commas, which the current pipeline cannot handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
